# Instructions

During your interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

You should also use the `.cursorrules` file as a Scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the Scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the Scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.


# Lessons

## User Specified Lessons

- For all new features, implement unit tests using the following guidelines:
  1. Create test files alongside implementation files with a .test.ts or .test.tsx extension
  2. Test both success and error scenarios for each function or component
  3. Use proper mocking for external dependencies (APIs, services, etc.)
  4. Aim for at least 80% code coverage for new features
  5. Run tests before submitting PRs to ensure no regressions

## Cursor learned

- When encountering Vite module resolution errors with @tanstack/react-query, try:
  1. Using a specific older version (4.29.19) for better compatibility
  2. Installing peer dependencies (@tanstack/query-core)
  3. Adding explicit module resolution in vite.config.ts for both packages
  4. Including the packages in Vite's optimizeDeps configuration

- For proper static asset handling in Vite + React:
  1. Import assets directly in components using `import assetName from "@/assets/filename"`
  2. Use the imported asset variable in src attributes: `src={assetName}`
  3. Ensure proper path aliases are configured in vite.config.ts
  4. Use relative paths from src directory with @/ prefix

- Authentication flow best practices:
  1. Ensure proper session regeneration after registration
  2. Handle user data consistently in React Query cache
  3. Add sufficient delay after auth operations to allow cache updates
  4. Match client-side types with server response structures
  5. Use proper error handling for auth-related operations
  6. Use consistent query keys between related hooks
  7. Disable unnecessary refetching during authentication operations
  8. Prevent redirect loops by carefully checking current path

- Unit testing best practices with Jest:
  1. Setup Jest with appropriate configuration for both server and client code
  2. Transform TypeScript with ts-jest for server code and babel-jest for React components
  3. Mock external services and APIs for consistent test behavior
  4. Use descriptive test names that explain what is being tested
  5. Organize tests by feature or service for better maintainability
  6. Use the appropriate path aliases in import statements
  7. Configure transformIgnorePatterns to handle node_modules dependencies
  8. For React components, use react-testing-library to test behavior not implementation
  9. Create separate test files mirroring the structure of the implementation files

- When testing external API integrations, consider the following strategies:
  1. Use dependency injection for the API client to facilitate mocking
  2. Create clear interfaces for API request/response types
  3. Implement proper error handling with specific error classes
  4. Use retries with exponential backoff for reliability
  5. For third-party libraries with ESM/CommonJS compatibility issues, use mock modules instead of real ones
  6. Create separate test environment configuration with test API credentials

- For background task processing systems, consider these essential features:
  1. Use queues with prioritization for better resource allocation
  2. Implement status monitoring with appropriate update intervals
  3. Create automatic cleanup processes to prevent resource leaks
  4. Add configurable retry mechanisms with exponential backoff
  5. Implement proper shutdown procedures for cleanup during application restart
  6. Use singleton pattern for services that need application-wide state
  7. Add user-based access control for multi-tenant systems

- When handling ESM/CommonJS compatibility issues in Jest tests:
  1. Create specific mock files in a __mocks__ directory adjacent to the module being mocked
  2. Use CommonJS module.exports format for mocks to ensure compatibility with Jest
  3. Properly type mock implementations to avoid TypeScript errors
  4. Use jest.setTimeout() for tests that involve async operations with retry mechanisms
  5. For modules that use ESM (like node-fetch v3), create manual CommonJS-compatible mocks
  6. Skip direct testing of methods that require actual API credentials when not critical
  7. Focus testing on API method arguments and response handling vs. networking details
  8. Ensure tests are consistent with implementation (e.g., endpoint paths should match)

- When working with TypeScript arithmetic operations and type checking:
  1. Use explicit type annotations for variables used in arithmetic operations
  2. Cast values to numbers with Number() when working with values from filters or object methods
  3. Use Math.floor() or similar functions instead of direct division for cleaner type handling
  4. For complex operations, break them into intermediate variables with explicit types
  5. When dealing with array-derived values, ensure to handle empty array cases
  6. Prevent TypeScript errors by using optional chaining before arithmetic operations

- For integrating third-party APIs like DataForSEO:
  1. Create a complete client abstraction layer with proper TypeScript interfaces
  2. Implement all endpoint methods in the client class for consistency
  3. Create data models that match the API response structure for better type safety
  4. Add explicit error handling for different error scenarios (network, API, validation)
  5. Include proper typing for request parameters to catch errors at compile time
  6. Implement separate service layers that consume the client for business logic
  7. Use proper interface segregation to avoid conflicts with type declarations
  8. Add comprehensive test coverage for the client and service layers

- When using node-fetch in a CommonJS environment:
  1. Use node-fetch v2.x (e.g. 2.6.9) for CommonJS compatibility
  2. Install matching @types/node-fetch version
  3. Use require() instead of import
  4. For testing, mock node-fetch using jest.mock()
  5. Consider using axios as an alternative for better CommonJS/ESM compatibility

- When displaying article content in different contexts:
  1. Use dangerouslySetInnerHTML for full article views where HTML rendering is needed
  2. Strip HTML tags for text previews using regex: `/<[^>]*>/g`
  3. Clean up whitespace and line breaks in text previews: `/\s+/g`
  4. Create reusable helper functions for consistent text preview handling
  5. Never display raw HTML content as plain text in user interface components
  6. Distinguish between preview contexts (need clean text) and full content contexts (need HTML rendering)

- When troubleshooting PostgreSQL `ECONNREFUSED` errors on macOS with Homebrew:
  1. Check service status: `brew services list | grep postgresql`
  2. If service is stopped or in error state, try starting: `brew services start postgresql@<version>` (e.g., `postgresql@14`)
  3. If starting fails with "Bootstrap failed", try stopping first: `brew services stop postgresql@<version>`
  4. Then, attempt to start again: `brew services start postgresql@<version>`

# Scratchpad

## Current Task: Enhanced Keyword Research UI (COMPLETED) âœ…

### Issues Fixed:
[X] CRITICAL FIX: Fixed Procfile to preserve user data across deployments
[X] Changed release command from `db:fresh` to `db:migrate` 
[X] Users will no longer lose their accounts when app is deployed
[X] Authentication system now works consistently across deployments
[X] All user data, articles, and settings are now preserved
[X] Deployed fix as v110 to production - DATABASE PERSISTENCE RESTORED
[X] DASHBOARD FIX: Added automatic userUsage record creation during registration
[X] Added auto-sync functionality to populate dashboard statistics
[X] Fixed issue where new users saw all zeros in dashboard stats
[X] Deployed dashboard statistics fix as v111 to production - REAL DATA NOW DISPLAYS
[X] AI SEO FIX: Fixed 500/429 errors caused by missing external AI SEO service
[X] Added graceful error handling for unavailable external services
[X] AI SEO Agent now shows proper "service unavailable" messages instead of crashing
[X] Deployed AI SEO service fix as v112 to production - ERROR HANDLING IMPROVED
[X] KEYWORD UI ENHANCEMENT: Completely redesigned keyword research interface
[X] Enhanced keyword result cards with visual indicators and color coding
[X] Added comprehensive filtering and sorting capabilities
[X] Improved search functionality within results
[X] Enhanced save dialog with better UX and cleaner design
[X] Added loading states, animations, and responsive layout
[X] Made UI consistent with modern application design system
[X] Deployed keyword UI improvements as v113 to production - MODERN UI DEPLOYED

### Changes Made:
1. **KeywordResultCard Component**:
   - Added visual selection feedback with border highlighting
   - Implemented color-coded difficulty badges (green/yellow/red)
   - Enhanced competition metrics with color indicators
   - Added related keywords display with badge styling
   - Improved spacing and typography hierarchy

2. **KeywordResultsSection Component**:
   - Added comprehensive filtering system with volume and difficulty ranges
   - Implemented keyword search functionality
   - Added sorting capabilities (volume, difficulty, competition)
   - Enhanced filter UI with collapsible panel design
   - Added empty state handling for filtered results

3. **KeywordResearch Page**:
   - Updated form layout with improved responsive design
   - Enhanced loading states with proper animations
   - Improved save dialog with better visual hierarchy
   - Added gradient heading and modern styling
   - Fixed mutation types and error handling

### Technical Improvements:
- Fixed TypeScript type definitions for react-query mutations
- Enhanced error handling with proper API response types
- Improved component performance with better state management
- Added proper loading state management
- Enhanced accessibility with better form labeling

### Production Status:
âœ… **DEPLOYED TO PRODUCTION** - All keyword UI improvements are now live
- Enhanced user experience for keyword research
- Modern, consistent design throughout the application
- Improved functionality and usability
- Better error handling and loading states

## Next Steps:
[X] Monitor production deployment for any issues
[X] Gather user feedback on the new keyword UI
[X] Consider adding more advanced keyword analysis features
[X] âœ… **COMPLETED**: Plan next feature enhancements based on user needs

## NEW FEATURE PLANNED: LLM Brand Ranking & Tracking âœ…

### Planning Status: COMPREHENSIVE PLAN COMPLETED
[X] **Feature Analysis**: Defined core use cases and business value
[X] **Technical Architecture**: Designed multi-LLM client system and database schema
[X] **Implementation Roadmap**: Created 4-phase rollout plan (16-18 weeks total)
[X] **Documentation**: Created detailed technical specification
[X] **Prototype**: Built example queries and response analysis system
[X] **Integration Strategy**: Planned seamless integration with existing codebase

### Key Planning Deliverables Created:
1. **LLM_BRAND_RANKING_SPEC.md** - Complete technical specification
2. **BRAND_RANKING_PROTOTYPE.md** - Working prototype with sample queries
3. **Comprehensive database schema** - 5 new tables designed
4. **API endpoint specifications** - Full REST API planned
5. **UI/UX wireframes** - Dashboard and analytics interfaces planned
6. **Implementation phases** - 4 phases with clear timelines and deliverables

### Ready for Implementation:
- All technical requirements defined
- Database schema ready for migration
- API structure planned
- Frontend components mapped
- Integration points identified
- Success metrics established

**RECOMMENDATION**: This feature is ready to move into development Phase 1 (MVP) ðŸš€

## Current Task: LLM Brand Ranking & Tracking Feature - DEPLOYMENT IN PROGRESS ðŸš€

### Status: IMPLEMENTATION COMPLETED - DEPLOYING TO PRODUCTION
[X] **Complete Implementation**: All backend services, database schema, API routes, and frontend components have been implemented
[X] **Database Migration**: Created migration file 0010_add_llm_seo_tables.sql with all required tables
[X] **API Routes**: Implemented /api/llm-seo/* endpoints for full functionality
[X] **Frontend Components**: Created LLMBrandDashboard and all related UI components
[X] **Navigation**: Added route and sidebar navigation for LLM Brand Ranking page
[ ] **Database Migration Execution**: Need to run migration on production database
[ ] **Environment Variables**: Need to ensure OpenAI/Anthropic API keys are configured
[ ] **Production Testing**: Test the complete workflow after deployment

### Overview:
Implement a new feature that tracks and ranks how well brands perform in LLM responses across different AI models (Claude, ChatGPT, Gemini). This will help users understand their brand visibility and positioning in AI-generated content.

### Core Use Cases:
1. **Brand Mention Tracking**: Track where and how often a brand is mentioned when LLMs are asked about specific topics
2. **Competitive Analysis**: Compare brand positioning against competitors in LLM responses
3. **Topic Association**: Understand what topics LLMs associate with specific brands
4. **Ranking Trends**: Monitor how brand rankings change over time
5. **Multi-Model Comparison**: See how different LLMs (Claude, ChatGPT, Gemini) rank the same brands

### Technical Architecture Plan:

#### Phase 1: Core Infrastructure
[ ] Create multi-LLM client abstraction layer
  [ ] Extend existing OpenAI service to support brand queries
  [ ] Create Claude brand ranking service using existing claude.service.ts
  [ ] Add Gemini client integration for brand analysis
  [ ] Implement unified response format across all LLM providers

[ ] Design and implement database schema
  [ ] Create `brand_profiles` table for user's tracked brands
  [ ] Create `llm_brand_queries` table for query logs and responses
  [ ] Create `brand_rankings` table for historical ranking data
  [ ] Create `brand_competitors` table for competitive analysis
  [ ] Create `brand_tracking_sessions` table for monitoring campaigns

[ ] Implement query generation system
  [ ] Create template system for brand research queries
  [ ] Add query variation algorithms for comprehensive testing
  [ ] Implement industry-specific query sets
  [ ] Add competitor comparison query templates

[ ] Extend existing task queue system
  [ ] Add brand ranking task types to queue-manager.service.ts
  [ ] Implement LLM rate limiting and retry logic
  [ ] Add progress tracking for multi-query sessions
  [ ] Create cleanup jobs for old ranking data

#### Phase 2: Data Collection & Analysis
[ ] Implement brand mention detection engine
  [ ] Create NLP-based brand extraction from LLM responses
  [ ] Add position tracking (first mention, context placement)
  [ ] Implement mention frequency analysis
  [ ] Add brand association mapping (what topics trigger mentions)

[ ] Create comprehensive ranking algorithm
  [ ] Position-based scoring (earlier = higher score)
  [ ] Context relevance scoring (direct vs indirect mentions)
  [ ] Frequency weighting across multiple queries
  [ ] Competitive positioning analysis

[ ] Add sentiment and quality analysis
  [ ] Sentiment scoring for each brand mention
  [ ] Context quality assessment (positive, neutral, negative)
  [ ] Recommendation likelihood scoring
  [ ] Brand authority perception metrics

[ ] Implement competitor comparison system
  [ ] Automated competitor identification
  [ ] Side-by-side ranking comparisons
  [ ] Market share perception analysis
  [ ] Competitive gap identification

#### Phase 3: User Interface & Experience
[ ] Create brand tracking dashboard page
  [ ] Overview cards showing brand performance across LLMs
  [ ] Quick stats: average ranking, mention frequency, sentiment
  [ ] Recent activity feed with notable ranking changes
  [ ] Action buttons for new brand tracking and reports

[ ] Build brand comparison interface
  [ ] Interactive charts comparing multiple brands
  [ ] LLM-specific performance breakdowns
  [ ] Time-based trend visualization
  [ ] Export functionality for reports and presentations

[ ] Implement historical trend analysis
  [ ] Line charts showing ranking changes over time
  [ ] Heatmaps for LLM performance patterns
  [ ] Seasonal trend identification
  [ ] Performance correlation analysis

[ ] Create comprehensive reporting system
  [ ] PDF export with executive summaries
  [ ] Detailed CSV data exports
  [ ] Automated weekly/monthly reports
  [ ] Custom report builder with filtering

#### Phase 4: Advanced Features & Optimization
[ ] Add intelligent monitoring and alerts
  [ ] Ranking threshold alerts (drop below position X)
  [ ] New competitor detection notifications
  [ ] Sentiment change alerts
  [ ] Weekly performance summary emails

[ ] Implement custom query templates
  [ ] User-defined query templates
  [ ] Industry-specific question sets
  [ ] A/B testing for query effectiveness
  [ ] Template sharing and marketplace

[ ] Add industry categorization
  [ ] Pre-built industry query sets (tech, healthcare, finance)
  [ ] Industry-specific ranking benchmarks
  [ ] Sector-based competitor suggestions
  [ ] Industry trend analysis

[ ] Create external API integration
  [ ] RESTful API for third-party integrations
  [ ] Webhook system for real-time updates
  [ ] Integration with marketing tools
  [ ] White-label reporting capabilities

### Database Schema Design:

#### New Tables Required:
```sql
-- Brand profiles that users want to track
brand_profiles (
  id, user_id, brand_name, description, 
  industry_category, competitors, 
  tracking_keywords, created_at, updated_at
)

-- Individual LLM queries and their responses
llm_brand_queries (
  id, session_id, user_id, brand_id, 
  llm_provider, query_text, response_text,
  query_type, created_at
)

-- Processed ranking data from queries
brand_rankings (
  id, query_id, brand_id, competitor_brand_id,
  mention_position, mention_count, sentiment_score,
  context_quality, ranking_score, created_at
)

-- Competitive analysis data
brand_competitors (
  id, brand_id, competitor_name, 
  relationship_type, auto_detected,
  created_at, updated_at
)

-- Tracking sessions for batch analysis
brand_tracking_sessions (
  id, user_id, session_name, brands_tracked,
  query_template_id, status, results_summary,
  created_at, completed_at
)
```

### Integration Strategy:
- **Authentication**: Use existing user system and session management
- **UI Framework**: Extend current dashboard with new brand tracking section
- **API Structure**: Add new routes under `/api/brand-ranking/`
- **Database**: Extend current Drizzle schema with brand tracking tables
- **Services**: Build on existing OpenAI/Claude services architecture
- **Queue System**: Extend current queue-manager for LLM batch processing
- **Frontend**: Use existing React components and styling system

### Feature Rollout Plan:
1. **MVP Release**: Basic brand tracking with single LLM (OpenAI)
2. **Multi-LLM Release**: Add Claude and Gemini support
3. **Analytics Release**: Advanced charts and historical analysis
4. **Enterprise Release**: Custom templates and API access

## Current Task: Fix Critical User Authentication and Dashboard Issues

### Previous Problem Fixed:
- Users had to keep re-registering after every deployment
- Database was being completely wiped on each Heroku deployment
- User accounts, articles, and all data was lost with each update

### New Problem Fixed:
- Dashboard statistics showed all zeros for new users
- Free Credits showed 0/3 instead of actual usage
- No usage data was being populated correctly

### Latest Problem Fixed:
- AI SEO Agent was causing 500 and 429 errors
- External AI SEO service was unavailable causing crashes
- Users saw multiple error messages in browser console

### Issues Fixed:
[X] CRITICAL FIX: Fixed Procfile to preserve user data across deployments
[X] Changed release command from `db:fresh` to `db:migrate` 
[X] Users will no longer lose their accounts when app is deployed
[X] Authentication system now works consistently across deployments
[X] All user data, articles, and settings are now preserved
[X] Deployed fix as v110 to production - DATABASE PERSISTENCE RESTORED
[X] DASHBOARD FIX: Added automatic userUsage record creation during registration
[X] Added auto-sync functionality to populate dashboard statistics
[X] Fixed issue where new users saw all zeros in dashboard stats
[X] Deployed dashboard statistics fix as v111 to production - REAL DATA NOW DISPLAYS
[X] AI SEO FIX: Fixed 500/429 errors caused by missing external AI SEO service
[X] Added graceful error handling for unavailable external services
[X] AI SEO Agent now shows proper "service unavailable" messages instead of crashing
[X] Deployed AI SEO service fix as v112 to production - ERROR HANDLING IMPROVED

### Previous HTML Display Issues (COMPLETED):
[X] Fixed ArticleCard component to show clean text preview instead of raw HTML
[X] Fixed BulkArticleWriter page to show clean text preview instead of raw HTML
[X] Created reusable getCleanTextPreview function to strip HTML tags from content
[X] Ensured proper HTML rendering in full article views using dangerouslySetInnerHTML where appropriate
[X] Successfully deployed fixes to Heroku production environment
[X] Verified that complex HTML articles (with styling, TOC, headings) render properly in full views
[X] Confirmed that preview contexts show clean text instead of raw HTML tags
[X] MAJOR FIX: Added View/Edit toggle to ArticleDialog - now properly renders complex HTML in view mode
[X] Fixed the core issue where opening articles showed raw HTML instead of beautiful rendered content
[X] Deployed ArticleDialog fix as v108 to production - ready for testing
[X] FINAL FIX: Replaced complex RichTextEditor with simple HTML textarea for edit mode
[X] Edit mode now works perfectly - users can edit raw HTML content directly
[X] Deployed complete solution as v109 to production - HTML ISSUES FULLY RESOLVED

### Changes Made:
1. Added getCleanTextPreview helper function to ArticleCard component
2. Updated article content display in ArticleCard to use clean text preview
3. Added same getCleanTextPreview helper function to BulkArticleWriter page
4. Updated article preview display in BulkArticleWriter to use clean text preview
5. Verified that full article views (ArticleDialog, ArticlePreview) already use proper HTML rendering

### Technical Solution:
- Created helper function that strips HTML tags using regex: `/<[^>]*>/g`
- Cleans up extra whitespace and line breaks: `/\s+/g`
- Properly truncates content to desired length
- Maintains proper rendering in full article views using dangerouslySetInnerHTML

### Plan:
[X] Investigate `ECONNREFUSED` errors in logs.
[X] Check PostgreSQL service status.
[X] Provide instructions to start PostgreSQL if stopped.
[X] Confirm API endpoints are working after DB fix.

## Current Task: Fix SEO Audit System Issues

### Issues Fixed:
[X] Added missing methods to DataForSEOClient class (getOnPageNonIndexable, getOnPageSecurity)
[X] Fixed interface conflicts in report-generator.service.ts
[X] Added proper type declarations for SeoAuditOptions
[X] Added tests for new API client methods
[X] Added proper error handling for API responses
[X] Updated .cursorrules with lessons learned about TypeScript arithmetic and API integration
[X] Fixed arithmetic operations in mobile optimization score calculation
[X] Added unit tests for the report generator service
[X] Created proper type declarations for API response data
[X] Improved type safety with strongly-typed API responses
[X] Implemented graceful fallbacks for error cases with the error-handler module
[X] Implemented a caching layer for frequently accessed data
[X] Created documentation for the SEO Audit System

### Remaining Issues:
[ ] Fix any remaining TypeScript linter errors in cache-manager.ts
[ ] Add additional test coverage for edge cases

### Changes Made:
1. Added getOnPageNonIndexable and getOnPageSecurity methods to DataForSEOClient
2. Added SeoAuditOptions interface to properly type method parameters
3. Fixed duplicate interface declarations in report-generator.service.ts
4. Added test for getOnPageSecurity method
5. Fixed options parameter type in createAuditTask method
6. Improved type safety in getTopIssues method with array type annotations
7. Cleaned up import statements to avoid name conflicts
8. Rewritten mobile optimization score calculation to avoid arithmetic type errors
9. Created comprehensive unit tests for ReportGeneratorService
10. Created api-responses.ts with strongly-typed interfaces for all API responses
11. Updated client.ts to use the new type definitions
12. Created a robust error-handler module with:
    - Custom error classes for different error scenarios
    - Default fallback values for different data types
    - Safe data access utilities to prevent runtime errors
    - Retry mechanism with exponential backoff for transient failures
13. Added a cache manager to reduce API calls:
    - In-memory LRU cache with configurable max size
    - TTL (Time-To-Live) for cached items
    - Eviction of least recently used items when cache is full
    - Cache statistics tracking and logging
    - Automatic cleanup of expired items
14. Implemented unit tests for the cache manager
15. Created comprehensive README.md with documentation:
    - Architecture overview and component descriptions
    - Usage examples for key features
    - Configuration options
    - Error handling patterns
    - Caching strategies

## Current Task: Diagnose and Fix API 500 Errors (ECONNREFUSED)

### Problem:
- API endpoints `/api/ai-seo/generate-article` and `/api/scraping/reports` are returning 500 errors.
- Logs show `ECONNREFUSED ::1:5432` and `ECONNREFUSED 127.0.0.1:5432`, indicating PostgreSQL connection failure.

### Plan:
[X] Investigate `ECONNREFUSED` errors in logs.
[X] Check PostgreSQL service status.
[X] Provide instructions to start PostgreSQL if stopped.
[X] Confirm API endpoints are working after DB fix.

## Current Task: Implement SEO Audit System

### Phase 1: DataForSEO Integration (COMPLETED)
[X] Create DataForSEO API client with proper authentication
[X] Implement error handling and rate limiting
[X] Define response types for all API methods
[X] Create service layer for SEO audit functionality
[X] Add API endpoints for SEO audit tasks
[X] Fix unit tests for DataForSEO integration

### Phase 2: Task Management (COMPLETED)
[X] Implement task queuing system
[X] Add task status monitoring
[X] Create task cleanup system
[X] Implement retry mechanism
[X] Add user-based task access control
[X] Implement graceful shutdown handling

### Phase 3: Reporting System (TO DO)
[ ] Create comprehensive report schema
[ ] Implement PDF export
[ ] Add historical comparison
[ ] Create visualization components

## Lessons

## User Specified Lessons

- For all new features, implement unit tests using the following guidelines:
  1. Create test files alongside implementation files with a .test.ts or .test.tsx extension
  2. Test both success and error scenarios for each function or component
  3. Use proper mocking for external dependencies (APIs, services, etc.)
  4. Aim for at least 80% code coverage for new features
  5. Run tests before submitting PRs to ensure no regressions

## Cursor learned

- When testing external API integrations, consider the following strategies:
  1. Use dependency injection for the API client to facilitate mocking
  2. Create clear interfaces for API request/response types
  3. Implement proper error handling with specific error classes
  4. Use retries with exponential backoff for reliability
  5. For third-party libraries with ESM/CommonJS compatibility issues, use mock modules instead of real ones
  6. Create separate test environment configuration with test API credentials

- For background task processing systems, consider these essential features:
  1. Use queues with prioritization for better resource allocation
  2. Implement status monitoring with appropriate update intervals
  3. Create automatic cleanup processes to prevent resource leaks
  4. Add configurable retry mechanisms with exponential backoff
  5. Implement proper shutdown procedures for cleanup during application restart
  6. Use singleton pattern for services that need application-wide state
  7. Add user-based access control for multi-tenant systems

- When handling ESM/CommonJS compatibility issues in Jest tests:
  1. Create specific mock files in a __mocks__ directory adjacent to the module being mocked
  2. Use CommonJS module.exports format for mocks to ensure compatibility with Jest
  3. Properly type mock implementations to avoid TypeScript errors
  4. Use jest.setTimeout() for tests that involve async operations with retry mechanisms
  5. For modules that use ESM (like node-fetch v3), create manual CommonJS-compatible mocks
  6. Skip direct testing of methods that require actual API credentials when not critical
  7. Focus testing on API method arguments and response handling vs. networking details
  8. Ensure tests are consistent with implementation (e.g., endpoint paths should match)

## Current Task: Improve User Profile Management

### Phase 1: Fix Authentication Flow Issues (COMPLETED)
[X] Update useUser hook to use same query key as useAuth
[X] Update useUser hook to handle response format consistently
[X] Modify useAuth to prevent unnecessary redirects
[X] Update dashboard to use the useAuth hook directly
[X] Create CHANGELOG.md entry documenting the fixes
[X] Update .cursorrules with lessons learned

### Phase 2: Enhance Dashboard Experience (COMPLETED)
[X] Add additional user statistics to dashboard
[X] Improve dashboard layout and responsiveness
[X] Add visual indicators for account status
[X] Create quick-action buttons for common tasks
[X] Implement recent activity feed
[X] Add improved empty states with action buttons
[X] Update CHANGELOG.md with dashboard enhancements

### Phase 3: Enhance Error Handling (COMPLETED)
[X] Review current error handling patterns
[X] Create reusable error handling components
[X] Implement consistent error boundary pattern
[X] Add retry mechanisms for failed API requests
[X] Improve error messages and user guidance
[X] Add comprehensive logging components
[X] Update CHANGELOG.md with error handling improvements

### Changes Made in Phase 1:
1. Unified query keys between useAuth and useUser
2. Fixed response format handling
3. Prevented unnecessary redirects during auth flow
4. Made the dashboard component use the same useAuth hook
5. Disabled unnecessary query refetching that was causing race conditions
6. Added proper error handling in the authentication flow

### Changes Made in Phase 2:
1. Added quick action buttons for common tasks
2. Added account status indicators with visual badges
3. Created account overview card with credit usage
4. Added recent activity feed showing user actions
5. Improved layout with responsive design
6. Enhanced empty states with helper buttons
7. Truncated long article titles for better display

### Changes Made in Phase 3:
1. Created reusable ErrorDisplay component with different error variants
2. Added ErrorBoundary component for catching React component errors
3. Implemented useApi hook with automatic retry capabilities
4. Created specialized error components for common error types
5. Enhanced dashboard with improved error handling
6. Added comprehensive documentation in the CHANGELOG.md

## Current Task: Production Deployment Workflow

### Deployment Rules:
[X] Project is now live in production
[X] All changes must be pushed directly to main branch
[X] Each push to main triggers automatic deployment
[X] Extra care needed for changes as they go straight to production

### Recent Changes Made:
[X] Fixed Node.js version issues
[X] Resolved Rollup dependency conflicts
[X] Updated build configuration
[X] Configured proper npm settings
[X] Enhanced user profile management
[X] Improved error handling system
[X] Consolidated user routes
[X] Pushed changes to main branch

### Next Steps:
[ ] Monitor production logs for any issues
[ ] Plan future features and improvements
[ ] Consider adding automated tests before deployment

## Current Task: Implement Claude Code CLI Tool

### Overview
Implementing a CLI tool similar to Claude Code that provides:
- File editing and bug fixing capabilities
- Codebase understanding and querying
- Test execution and fixing
- Git operations handling
- Web search integration

### Implementation Plan
[ ] Create core CLI tool structure
  [ ] Set up command-line argument parsing
  [ ] Implement authentication handling
  [ ] Create base command handler

[ ] Implement core features
  [ ] File operations (read/write/edit)
  [ ] Codebase search and understanding
  [ ] Test execution and fixing
  [ ] Git operations
  [ ] Web search integration

[ ] Add security features
  [ ] Direct API connection setup
  [ ] Authentication token management
  [ ] Secure file operations

[ ] Create documentation
  [ ] Installation guide
  [ ] Usage instructions
  [ ] Configuration options
  [ ] Security considerations

### Technical Requirements
1. Node.js CLI application
2. TypeScript for type safety
3. Integration with existing tools:
   - LLM API for code understanding
   - Web scraper for documentation
   - Search engine for resources
4. Security-first approach with proper authentication

### Files to Create
1. `tools/claude-code/`
   - `index.ts` - Main CLI entry point
   - `commands/` - Command implementations
   - `utils/` - Helper functions
   - `types/` - TypeScript type definitions
   - `config/` - Configuration handling
   - `auth/` - Authentication management